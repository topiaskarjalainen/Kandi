\section{Yleisiä käytäntöjä MCMC-menetelmissä}

Usein on tapana simuloida useampi kuin yksi ketju kustakin tapauksesta. Tällöin asetetaan näiden ketjujen aloitustilat eriäviksi, jotta saadaan vähennettyä aloitustilan vaikutusta lopputukokseen.

Toinen yleinen käytäntö on jättää ketjun alkupäästä jonkin verran tiloja huomiotta, sillä ketjun alkupäässä ei se ei välttämättä ole vielä saavuttanut tasapainojakaumaa. Tätä kutsutaan \textit{Burn-in periodiksi}.

Joskus taas on hyvä tapa pudottaa joka $n$:s tila ketjusta. Tilojen tiputtaminen ei vaikuta tasapainojakaumaan, kunhan ketju vain on saavuttanut sen. Tilojen tiputtamista on hyötyä jos mallissa on paljon parametreja, jolloin tietokoneessa voi tulla ongelmia tilan kanssa.

\section{Konvergenssi ja Diagnostiikka}

Kappaleissa \ref{gibbs} ja \ref{Metropolis--Hastings algoritmi} esiteltyjen algoritmien kohdalla voi herätä kysymys, että mikä on riittävä määrä iteraatioita, jotta Markovin ketju on saavuttanut tasapainojakaumansa ja otanta on riittävän hyvä aproksimaatio posteriorijaukaumasta. Esimerkiksi kun katsotaan kuvan \ref{kuva1} vasemman puolen kuvan punaista polkua, niin voimme sanoa, että se ei ole vielä saavuttanut tasapainojakaumaa sillä tunnemme melko hyvin halutun jakauman, mutta yleensä emme välttämättä osaa sanoa tätä suoraan. 

Toinen yleinen ongelma, joka kaipaa pohdintaa on se, että ketjujen sisällä on korrelaatiota, mikä vaikeuttaa päättelyä simuloinnin tuloksista. Otannat eivät siis ole välttämättä täysin riippumattomia.

\subsection{Gelmanin $\hat{R}$}

Yksi yleisimmistä estimaattoreista, joita käytetään MCMC-metodeissa Markovin ketjujen konvergenssin arvioimiseen on \textit{Andrew Gelmanin} $\hat{R}$ \cite{gelman_andrew_bayesian_nodate}. Se mittaa ketjujen sisäistä sekoittumista (eng. \textit{mixing}) ja erillisten ketjujen välistä sekoittumista.
\begin{maar}
\textit{Gelmanin $\hat{R}$} \cite{gelman_andrew_inference_1992}\cite{gelman_andrew_general_1998} lasketaan jakamalla ensin jokin määrä simuloituja ketjua erillisillä aloituspisteillä keskeltä kahtia. Olkoon nyt $m$ ketjujen määrä jaon jälkeen ja $n$ ketjujen pituus. Olkoot
	$\psi_{ij} (i=1,...,n;j=1,...,m)$ tila $i$ ketjussa $j$.  Nyt merkataan
\begin{subequations}
	\begin{equation}
		B = \frac{n}{m-1}\sum_{j=1}^{m}(\overline{\psi}_{.j}-\overline{\psi}_{..})^2, \hspace{0.3cm}\text{jossa}
	\end{equation}
	\begin{equation}
		\overline{\psi}_{.j} = \frac{1}{n}\sum_{i=1}^{n} \psi_{ij}
	\end{equation}
	\begin{equation}
		\overline{\psi}_{..} = \frac{1}{m}\sum_{j=1}^{m} \overline{\psi}_{.j}
	\end{equation}
\end{subequations}
Ja merkataan 
\begin{equation}
	W = \frac{1}{m}\sum_{j=1}^{m}s_j^2, \hspace{0.2cm} jossa \hspace{0.2cm}
	s_j^2 = \frac{1}{n-1}\sum_{i=1}^{n}(\psi_{ij}-\overline{\psi}_{.j})^2
\end{equation}
Jossa siis $B$ on ketjujen välinen varianssi (\textit{between sequence}) ja $W$ on ketjujen sisäinen varianssi (\textit{within sequence}). Näiden painotettuna keskiarvona saadaan estimaattori $\psi$:n marginaaliselle posteriori varianssille
\begin{equation}\label{sigma+}
	\hat{\sigma}^+(\psi|y) = \frac{n-1}{n}W + \frac{1}{n}B
\end{equation}
Nyt $\hat{R}$ voidaan määritellä
\begin{equation}
	\hat{R}= \sqrt{\frac{\hat{\sigma}^+(\psi|y)}{W}}
\end{equation}
\end{maar}

Gelmanin $\hat{R}$ mittaa sitä miten paljon kullakin hetkellä $\psi$:n jakauma voisi supistua, jos simulaatioiden annettaisiin jatkua loputtomasti. Eli siis kun $\hat{R}\approx 1$, niin voidaan sanoa, että Markovin ketju on todennäköisesti saavuttanut tasapainojakaumansa, sillä tällöin eri pisteitä aloitettujen ketjujen välinen varianssi on vakaantunut.

Tällä diagnostiikalla on kuitenkin onglemansa ja rajoitteensa. Se saattaa virheellisesti diagnosoida konvergoituneen ketjun esimerkiksi jos ketjulla on pitkät hännät. Sen takia käytetäänkin mielummin muita diagnostiikka keinoja. Esimerkiksi \textit{Stan} käyttää korjattua $\hat{R}$ diagnostiikkaa \cite{vehtari_2019}.

\subsection{ESS}

Toinen hyödyllinen diagnostiikka MCMC-menetelmissä on niin sanottu \textit{effective sample size (ESS)}, joka mittaa sitä kuinka monta effektiivisesti riippumatonta otosta ketjussa on. Tämä tarkoittaa siis sitä, että se mittaa kuinka paljon ketjun autokorrelaatio vaikuttaa keskivirheeseen verrattuna täysin riippumattomiin otoksiin.
\begin{maar}
	ESS lasketaan kaavalla
	\begin{equation}
		\hat{n}_{eff}=\frac{mn}{1+2\sum_{t=1}^{T}\hat{\rho}_t}
	\end{equation}
	jossa 
	\begin{subequations}
	\begin{equation}
		\hat{\rho}_t = 1 - \frac{V_t}{2\hat{\sigma}^+}
	\end{equation}
	\begin{equation}
		V_t=\frac{1}{m(n-t)}\sum_{j=1}^{m}\sum_{i=t+1}^{n}(\psi_{ij}-\psi_{i-t,j})^2
	\end{equation}
	\end{subequations}
	$\hat{\sigma}^+$ saadaan kaavasta \ref{sigma+}. Termissä $\sum_{t=1}^{T}\hat{\rho}_t$ autokorrelaatioita summataan, kunnes kahden peräkkäisen kovariaatin summa on negatiivinen \cite{geyer_practical_1992}.
\end{maar}















