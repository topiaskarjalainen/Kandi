\chapter{Markov Chain Monte Carlo}

\section{Gibbsin otanta-algoritmi}

\textit{Gibbsin otanta-algoritmi} on tapa simuloida Bayesiläistä moniulotteista posteriorijakaumaa (eli ulottuvuuksia vähintään 2), kun suora otanta on hankalaa. Algoritmi on nimetty amerikkalaisen fyysikon, \textit{Josiah Willard Gibbs}'n (1839-1903) mukaan, mutta sen todellinen kehittäjä on veljekset \textit{Donald Geman} (1943-) ja \textit{Stuart Geman} (1949-) vuonna 1984 artikkelissa \textit{Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images}

\begin{maar}
	Olkoot $\theta$ parametrivektori, joka jaetaan $d$:hen osaan tai osavektoriin, eli $\theta = (\theta_1, \theta_2,...,\theta_d)$. Gibbsin otanta-algoritmi määritellään seuraavanlaisesti:
	\begin{enumerate}
		\item Valitaan $\theta$:n osavektoreille järjestys.
		\item Arvotaan uusi tila jokaiselle osavektorille $\theta_j$ ehdollistamalla se jokaiselle muulle parametrille, eli vedetään arvot $\theta = (\theta_1, \theta_2,...,\theta_d)$ jakaumista
		\begin{equation}
			p(\theta_j|\theta_{-j,n-1},y)
		\end{equation}
		missä $\theta_{-j,n-1}$ on kaikki muut $\theta$:n komponentit paitsi $j$s komponentti, näiden tämänhetkisillä arvoilla eli 
		\begin{equation*}
			\theta_{-j,n-1} = (\theta_{1,n},...,\theta_{j-1,n},\theta_{j+1,n-1},...,\theta_{d,n-1})
		\end{equation*}
	\end{enumerate}
\end{maar}



\section{Metropolis--Hastings algoritmi}\label{Metropolis--Hastings algoritmi}

\textit{Metropolis--Hastings} algoritmi on kehittelijöidenssä \textit{Nicholas Metropolisksen} (1915-1999) ja \textit{Wilfred Keith Hastings}:n (1930-2016) mukaan nimetty MCMC-menetelmä, jolla voidaan simuloida Bayesiläisessä analyysissa käytettäviä posteriori jakaumia myös silloin kun tiheys on mahdotonta määrittää analyyttisesti.

Algoritmin pohjan kehitti \textit{Stanislav Ulam} ja \textit{Metropolis} työskennellessään \textit{Los Alamosissa} ja myöhemmin \textit{Metropolis} kehitteli nykyään \textit{Metropolis-algoritmina} tunnettua algoritmiä ja esittelivät sen artikkelissa \textit{Equation of state calculations by fast computing machines}\cite{metropolis_nicholas_equation_1953}. Tämä versio algoritmista vaati, että pian esiteltävä \textit{ehdotusjakauma} on symmetrinen. Myöhemmin \textit{Hastings} laajenti algoritmin koskemaan myös epäsymmetrisiä ehdotusjakaumia artikkelissa \textit{Monte Carlo Sampling Methods Using Markov Chains and Their Applications}

\begin{merk}
	TN-jakauma $J_n(\cdot|\cdot)$ on niin sanottu \textit{ehdotusjakauma} (\textit{proposal distribution, jumping distribution}), josta \textit{MH-algoritmissa} arvotaan ehdotus tila.
\end{merk}

\begin{maar}\label{mh-maar}
	Metropolis--Hastings algoritmi on seuraavanlainen
	\begin{enumerate}
		\item Valitaan aloitus tila $\theta_0$ ja asetetaan $n=0$
		\item Generoidaan kandidaatti tila $\theta'$ satunnaisesti jakaumasta $J_n(\theta'|\theta_{n-1})$
		\item Lasketaan tiheyksien tai todennäköisyyksien suhde
		\begin{displaymath}
			r = \frac{p(\theta'|y)/J_n(\theta'|\theta_{n-1})}{p(\theta_{n-1}|y)/J_n(\theta_{n-1}|\theta')}
		\end{displaymath}
		\item Asetetaan
		\begin{displaymath}
			\theta_t= 
			\begin{cases}
				\theta', \text{todennäköisyydellä} \hspace{0.3cm} \min(r,1) \\
				\theta_{t-1}, \text{muuten}
			\end{cases}
		\end{displaymath}
	\end{enumerate}
	Jossa $J_t(\theta'|\theta^{t-1})$ on ns. ehdotusjakauma (eng. proposal distribution).
\end{maar}

\begin{lause}
	Määritelmän \ref{mh-maar} algoritmi tuottaa Markovin ketjun jolla on uniikki tasapainojakauma, ja jonka tasapainojakauma on posteriorijakauma $p(\theta|y)$, jossa $y$ on data. 
\end{lause}

\begin{proof}

	Ohitamme todistuksen, että kyseessä Markovin ketju jolla yksi tasapainojakauma, mutta todistamme toisen osan, eli että tasapainojakauma on haluttu $p(\theta|y)$ eli posteriori jakauma. Todistus nojautuu Markovin ketjun kääntyvyysominaisuuteen (\ref{kaant-disk} ja \ref{kaant-jatk}), eli
	\begin{equation}\label{kaant-mcmc}
		T(\theta_{n}|\theta_{n-1})p(\theta_{n-1}|y) = T(\theta_{n-1}|\theta_{n})p(\theta_{n}|y)
	\end{equation}
	joka on siis riittävä ehto tasapainojakauman olemassaololle. Mietitään kahta tapausta: (1) $\theta_n \neq \theta_{n-1}$ ja (2) $\theta_n = \theta_{n-1}$.
	Tapauksen (2) siirtymä voi tapahtua kahdella tavalla. Joko kohdassa 4. ehdotus $\theta'$ hylätään, tai se hyväksytään, mutta osutaan sattumanvaraisesti takaisin samaan kohtaan. Kuitenkin selvästi nähdään, että ehto \ref{kaant-mcmc} pätee tilanteessa (2).
	
	Tilanteessa (1)Siirtymätodennäköisyys pisteestä $\theta_{n-1}$ pisteeseen $\theta_n$ on
	\begin{equation}
		T(\theta_n|\theta_{n-1}) = J_n(\theta_n|\theta_{n-1})
		\min\Big( \frac{p(\theta_n|y)J_n(\theta_{n-1}|\theta_n)}{p(\theta_{n-1}|y)J_n(\theta_{n}|\theta_{n-1})},1 \Big)
	\end{equation}
	Jota voidaan muokata helposti
	\begin{equation}\label{metr-has-proof1}
		\begin{split}
			T(\theta_n|\theta_{n-1}) &= J_n(\theta_n|\theta_{n-1})
		\min\Big( \frac{p(\theta_n|y)J_n(\theta_{n-1}|\theta_n)}{p(\theta_{n-1}|y)J_n(\theta_{n}|\theta_{n-1})},1 \Big) \\
		&= \frac{1}{p(\theta_{n-1}|y)} \min \Big( p(\theta_n|y)J_n(\theta_{n-1}|\theta_n) , p(\theta_{n-1}|y)J_n(\theta_n|\theta_{n-1})  \Big)
		\end{split}
	\end{equation}
	Nähdään kuitenkin, että yhtälön \ref{metr-has-proof1} alempi yhtäläisyys on symmetrinen eli
	
	\begin{equation}\label{symmetric}
		T(\theta_{n-1}|\theta_{n})=\frac{1}{p(\theta_{n}|y)} \min \Big( p(\theta_{n-1}|y)J_n(\theta_{n}|\theta_{n-1}) , p(\theta_{n}|y)J_n(\theta_{n-1}|\theta_{n})  \Big)
	\end{equation}
	joten kerrotaan \ref{metr-has-proof1} termillä $p(\theta_{n-1}|y)$ ja hyödynnetään \ref{symmetric} ominaisuutta
	\begin{equation*}
	\begin{split}
		T(\theta_n|\theta_{n-1})p(\theta_{n-1}|y) &= \frac{1}{p(\theta_{n-1}|y)} \min \Big( p(\theta_n|y)J_n(\theta_{n-1}|\theta_n) , p(\theta_{n-1}|y)J_n(\theta_n|\theta_{n-1})  \Big) p(\theta_{n-1}|y) \\
		&= \frac{1}{p(\theta_{n}|y)} \min \Big( p(\theta_{n-1}|y)J_n(\theta_{n}|\theta_{n-1}) , p(\theta_{n}|y)J_n(\theta_{n-1}|\theta_{n})  \Big) p(\theta_{n}|y) \\
		&= T(\theta_{n-1}|\theta_{n})p(\theta_{n}|y)
	\end{split}
	\end{equation*}
	Eli myös tapauksessa (1) yhtälö \ref{kaant-mcmc} pätee.
\end{proof}

\begin{esim}\label{mh-esim1}
	Ajatellaan kuvitteellista tapausta, jossa meillä jatkuva kaksiulotteinen todennäköisyysjakauma, jonka tiheysfunktio on 
	\begin{equation}
		p(\theta) \varpropto \exp(-5 |\theta_1^2+\theta_2^2-1|)
	\end{equation}
	joka muodostaa regasmaisen 2-ulotteisen jakauman. Valitaan ehdotusjakaumaksi $J_n(\theta_n|\theta_{n-1})$ 2d-multinormaalijakauma 
	\begin{equation}
		J_n(\theta_n|\theta_{n-1}) \sim N(\theta_{n-1}, \sigma^2 I_2)
	\end{equation}
	jossa $I_2$ on 2x2 yksikkömatriisi ja olkoot $\sigma^2 = 0.01$. Nyt Metropolis Hastings algoritmin avulla voidaan simuloida jakaumaa $p(\theta)$ algoritmilla \ref{mh-maar}. Simuloidaan kaksi Markovin ketjua asettemalla aloitustiloiksi $(0,0)$ ja $(5,5)$, 
	\begin{figure}[h!]
		\includegraphics[width=\textwidth]{mhexample1}
		\caption[Kaksiulotteinen Metropolis--Hastings esimerkki]{\textit{Vasemmalla: (5,5). Keskellä: (0,0) Oikealla: tiheysestimaatti (huomaa eri skaala).}}
		\label{kuva1}
	\end{figure}
	kummastakin 10 000 tilaa. Simuloimme myös 200 000 pistettä aloitusarvolla (0,0), joista luodaan tiheysestimaatti. Tulokset löytyy kuvasta \ref{kuva1}. Kahdessa ekassa kuvassa viiva on ensimmäisen 250 pisteen polku. Selvästi nähdään, että aloituspisteellä ei ole väliä. Markovin ketjun tasapainojakauma on sama huolimatta aloituspisteestä.
	
\end{esim}

\begin{esim}\label{MH-esim2}
	\begin{figure}[h!]
		\includegraphics[width=0.7\textwidth]{mhexample2}
		\caption[Yksiulotteinen Metropolis--Hastings esimerkki]{Esimerkin \ref{MH-esim2} tulokset}
		\label{kuva2}
	\end{figure}
	Otetaan toisena esimerkkinä klassinen diskreetti tapaus, jossa oletetaan, että havainnot ovat jauakautuneet \textit{Bernoulli-jakauman} mukaan, 
	$y_i \sim Bernoulli(\theta)$, 
	ja että priori on tasajakauma. Tällöin tiedetään, että analyyttinen posteriori on 
	\begin{equation*}
		p(\theta|y_i) = Beta(\sum_{i=1}^{n} y_i+1, n - \sum_{i=1}^{n} y_i+1)
	\end{equation*}
	Valitaan hieman eksoottinen ehdotusjakauma esimerkin vuoksi:
	\begin{equation}
		J_n(\theta_n|\theta_{n-1}) \sim \begin{cases}
			Unif(\theta_{n-1},1) & \text{kun } \theta_{n-1}<0.5 \\
			Unif(0,\theta_{n-1}) & \text{kun } \theta_{n-1}\geq0.5
		\end{cases}
	\end{equation}
	Toisin kuin esimerkissä \ref{mh-esim1}, nyt ehdotusjakauma ei olekkaan symmetrinen.
	
	Oletetaan että, meillä on havainnot $(1,1,1,0,0,1,0,0,0,0)$ ja tarkastellaan sekä analyyttistä että MH-algoritmin tuottamaa jakaumaa ja niiden eroja.

	Kuvasta \ref{kuva2} nähdään esimerkin tulokset. Huomataan, että vaikka ehdotusjakauma on melko kummallinen, niin kuitenkin tarpeaksi monella iteraatiolla saavutetaan tasapainojakauma. Huomaa, että vasemmassa kuvaajassa on vihreällä simulaatio keskiarvo, ja punaisella analyyttinen keskiarvo, mutta nämä arvot ovat niin lähellä toisiaan, että viivat ovat päällekkäin.
\end{esim}

\subsection{Ehdotusjakauman valinnasta}

Kummassakin kappaleen \ref{Metropolis--Hastings algoritmi} esimerkissä valitsimme ehdotusjakauman melko satunnaisesti. Varsinkin esimerkissä \ref{MH-esim2} se on erittäin epätavallinen, mistä syystä hyvän approksimaation saavuttaminen vie todella monta iteraatiota. Yleensä jos haluamme oikeasti tehokkaasti ja ekonomisesti simuloida jakaumia esitetyllä algoritmilla, haluamme valita ehdotusjakauman jollakin järkevällä, systemaattisella tavalla, joka minimoisi tarvittavien iteraatioiden määrän.

Yleisesti ottaen hyvällä ehdotusjakaumalla on muutama ominaisuus\cite{gelman_andrew_bayesian_nodate}
\begin{enumerate}
	\item Kaikilla $\theta$:n arvoilla on helppo arpoa arvo $J(\theta'|\theta)$
	\item Suhde $r$ on helppo laskea
	\item Siirtymät ovat tarpeaksi pitkiä. Muuten Markovin Ketju etenee liian hitaasti ja hyvän estimaatin saaminen kestää liian pitkään.
	\item Siirtymiä ei hylätä liian usein. Muuten Markovin Ketju ei etene vaan seisoo paikallaan.
\end{enumerate}

Lisäksi simulointia voidaan nopeuttaa mm. käyttämällä adaptiivista ehdotusjakaumaa, eli toisinsanoen ehdotusjakaumaa muunnellaan riippuen ketjun liikkeistä. Esimerkiksi jos ehdotusjakauman ydin on samaa muotoa ku

















