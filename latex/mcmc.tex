\chapter{Markov Chain Monte Carlo}

\section{Gibbsin otanta-algoritmi}\label{gibbs}

\textit{Gibbsin otanta-algoritmi} on tapa simuloida Bayesiläistä moniulotteista posteriorijakaumaa (eli ulottuvuuksia vähintään 2), kun suora otanta on hankalaa. Algoritmi on nimetty amerikkalaisen fyysikon, \textit{Josiah Willard Gibbs}'n (1839-1903) mukaan, mutta sen todellinen kehittäjä on veljekset \textit{Donald Geman} (1943-) ja \textit{Stuart Geman} (1949-) vuonna 1984 artikkelissa \textit{Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images}.

\begin{maar}\label{gibbs}
	Olkoot $\theta$ parametrivektori, joka jaetaan $d$:hen osaan tai osavektoriin, eli $\theta = (\theta_1, \theta_2,...,\theta_d)$. Gibbsin otanta-algoritmi määritellään seuraavanlaisesti:
	\begin{maar}\label{gibbs}
	Olkoot $\theta$ parametrivektori, joka jaetaan $d$:hen osaan tai osavektoriin, eli $\theta = (\theta_1, \theta_2,...,\theta_d)$. Gibbsin otanta-algoritmi määritellään seuraavanlaisesti:
	\begin{enumerate}
		\item Valitaan satunnainen indeksi $j$ jolle $1 \leq j \leq d$
		\item Arvotaan uusi tila jokaiselle osavektorille $\theta_j$ ehdollistamalla se jokaiselle muulle parametrille, eli vedetään arvot $\theta = (\theta_1, \theta_2,...,\theta_d)$ jakaumista
		\begin{equation}
			p(\theta_j|\tnm_{-j},y)
		\end{equation}
		missä $\tnm_{-j}$ on kaikki muut $\theta$:n komponentit paitsi $j$s komponentti, näiden tämänhetkisillä arvoilla eli 
		\begin{equation*}
			\tnm_{-j} = (\tn_{1},...,\tn_{j-1},\tnm_{j+1},...,\tnm_{d})
		\end{equation*}
	\end{enumerate}
\end{maar}
\end{maar}

Yleensä ylläolevassa määritelmässä kohdassa 1. ei arvota uutta järjestystä, vaan järjestys päätetään alussa, ja sitä pidetään koko algortimin ajan samana. Tällöin kyseessä on ns. \textit{systemic scan Gibbs sampler}.

\begin{lause}
	Määritelmän \ref{gibbs} mukaisen algoritmin tuottamalla Markovin ketjulla on tasapainojakauma $p(\theta)$. \cite{koistinen_computational_2009}
\end{lause}

\begin{proof}
	Olkoot $\tnm$ alkuperäienen tila, ja $\tn_j$ uusi $j$:nennen parametrin tila. Nyt $\tnm$:n ja $\tn_j$:n yhteis tiheys on 
	\begin{equation}
		p(\tnm)p_j(\tn_j|\tnm_{-j})
	\end{equation}
	Nyt voidaan integroida
	\begin{equation}
		\begin{split}
			\int p(\tnm)p_j(\tn_j|\tnm_{-j}) d\tnm_j &= \int p(\tnm_j|\tnm_{-j})p(\tnm_{-j})p_j(\tn_j|\tnm_{-j}) d\tnm_j \\
			&= p(\tnm_{-j})p_j(\tn_j|\tnm_{-j}) \int p(\tnm_j|\tnm_{-j}) d\tnm_j \\
			&= p(\tnm_{-j})p_j(\tn_j|\tnm_{-j}) \\
			&= p(\tn_j,\tnm_{-j})
		\end{split}
	\end{equation}
	Eli Gibbs otanta-algoritmin päivitys ei muuta jakaumaa. Nyt voidaan soveltaa lausetta \ref{cyclic-kernel}, jolloin voidaan todeta, että koska $p$ tasapainojakauma jokaiselle $p_j(\tn_j|\tnm_{-j})$, niin se on tasapainojakauma niiden yhteis siirtymätiheydelle
	\begin{equation}
		T = \prod_{j=1}^{d} p_j(\tn_j|\tnm_{-j})
	\end{equation}

\end{proof}

Ohitetaan Gibbsin otanta-algoritmin kohdalla toistaiseksi esimerkit, ja palataan siihen kappaleessa \ref{esimerkki}, jossa tarkastelemme laajempaa esimerkkiä lineaarisen regression parissa. Toteutamme tämän Gibbsin otanta-algoritmina.

\section{Metropolis--Hastings algoritmi}\label{Metropolis--Hastings algoritmi}

\textit{Metropolis--Hastings} algoritmi on kehittelijöidenssä \textit{Nicholas Metropolisksen} (1915-1999) ja \textit{Wilfred Keith Hastings}:n (1930-2016) mukaan nimetty MCMC-menetelmä, jolla voidaan simuloida Bayesiläisessä analyysissa käytettäviä posteriori jakaumia myös silloin kun tiheys on mahdotonta määrittää analyyttisesti.

Algoritmin pohjan kehitti \textit{Stanislav Ulam} ja \textit{Metropolis} työskennellessään \textit{Los Alamosissa} ja myöhemmin \textit{Metropolis} kehitteli nykyään \textit{Metropolis-algoritmina} tunnettua algoritmiä ja esittelivät sen artikkelissa \textit{Equation of state calculations by fast computing machines}\cite{metropolis_nicholas_equation_1953}. Tämä versio algoritmista vaati, että pian esiteltävä \textit{ehdotusjakauma} on symmetrinen. Myöhemmin \textit{Hastings} laajenti algoritmin koskemaan myös epäsymmetrisiä ehdotusjakaumia artikkelissa \textit{Monte Carlo Sampling Methods Using Markov Chains and Their Applications}

\begin{merk}
	TN-jakauma $J_n(\cdot|\cdot)$ on niin sanottu \textit{ehdotusjakauma} (\textit{proposal distribution, jumping distribution}), josta \textit{MH-algoritmissa} arvotaan ehdotus tila.
\end{merk}

\begin{maar}\label{mh-maar}
	Metropolis--Hastings algoritmi on seuraavanlainen
	\begin{enumerate}
		\item Valitaan aloitus tila $\theta_0$ ja asetetaan $n=0$
		\item Generoidaan kandidaatti tila $\theta'$ satunnaisesti jakaumasta $J_n(\theta'|\theta_{n-1})$
		\item Lasketaan tiheyksien tai todennäköisyyksien suhde
		\begin{displaymath}
			r = \frac{p(\theta'|y)/J_n(\theta'|\theta_{n-1})}{p(\theta_{n-1}|y)/J_n(\theta_{n-1}|\theta')}
		\end{displaymath}
		\item Asetetaan
		\begin{displaymath}
			\theta_t= 
			\begin{cases}
				\theta', \text{todennäköisyydellä} \hspace{0.3cm} \min(r,1) \\
				\theta_{t-1}, \text{muuten}
			\end{cases}
		\end{displaymath}
	\end{enumerate}
	Jossa $J_t(\theta'|\theta^{t-1})$ on ns. ehdotusjakauma (eng. proposal distribution).
\end{maar}

\begin{lause}
	Määritelmän \ref{mh-maar} algoritmi tuottaa Markovin ketjun jolla on uniikki tasapainojakauma $p(\theta)$ 
\end{lause}

\begin{proof}

Todistus nojautuu Markovin ketjun kääntyvyysominaisuuteen (\ref{kaant-disk} ja \ref{kaant-jatk}), eli
	\begin{equation}\label{kaant-mcmc}
		T(\tn|\tnm)p(\tnm) = T(\tnm|\tn)p(\tn)
	\end{equation}
	joka on siis riittävä ehto tasapainojakauman olemassaololle. Mietitään kahta tapausta: (1) $\tn \neq \tnm$ ja (2) $\tn = \tnm$.
	Tapauksen (2) siirtymä voi tapahtua kahdella tavalla. Joko kohdassa 4. ehdotus $\theta'$ hylätään, tai se hyväksytään, mutta osutaan sattumanvaraisesti takaisin samaan kohtaan. Kuitenkin selvästi nähdään, että ehto \ref{kaant-mcmc} pätee tilanteessa (2).
	
	Tilanteessa (1) siirtymätodennäköisyys pisteestä $\tnm$ pisteeseen $\tn$ on
	\begin{equation}
		T(\tn|\tnm) = J_n(\tn|\tnm)
		\min\Big( \frac{p(\tn)J_n(\tnm|\tn)}{p(\tnm)J_n(\tn|\tnm)},1 \Big)
	\end{equation}
	Jota voidaan muokata helposti
	\begin{equation}\label{metr-has-proof1}
		\begin{split}
			T(\tn|\tnm) &= J_n(\tn|\tnm)
		\min\Big( \frac{p(\tn)J_n(\tnm|\tn)}{p(\tnm)J_n(\tn|\tnm)},1 \Big) \\
		&= \frac{1}{p(\tnm)} \min \Big( p(\tn)J_n(\tnm|\tn) , p(\tnm)J_n(\tn|\tnm)  \Big)
		\end{split}
	\end{equation}
	Nähdään kuitenkin, että yhtälön \ref{metr-has-proof1} alempi yhtäläisyys on symmetrinen eli
	\begin{equation}\label{symmetric}
		T(\tnm|\tn)=\frac{1}{p(\tn)} \min \Big( p(\tnm)J_n(\tn|\tnm) , p(\tn)J_n(\tnm|\tn)  \Big)
	\end{equation}
	joten kerrotaan \ref{metr-has-proof1} termillä $p(\tnm)$ ja hyödynnetään \ref{symmetric} ominaisuutta
	\begin{equation*}
	\begin{split}
		T(\tn|\tnm)p(\tnm) &= \frac{1}{p(\tnm)} \min \Big( p(\tn)J_n(\tnm|\tn) , p(\tnm)J_n(\tn|\tnm)  \Big) p(\tnm) \\
		&= \frac{1}{p(\tn)} \min \Big( p(\tnm)J_n(\tn|\tnm) , p(\tn)J_n(\tnm|\tn)  \Big) p(\tn) \\
		&= T(\tnm|\tn)p(\tn)
	\end{split}
	\end{equation*}
	Eli myös tapauksessa (1) yhtälö \ref{kaant-mcmc} pätee.
\end{proof}

\begin{esim}\label{mh-esim1}
	Ajatellaan kuvitteellista tapausta, jossa meillä jatkuva kaksiulotteinen todennäköisyysjakauma, jonka tiheysfunktio on 
	\begin{equation}
		p(\theta) \varpropto \exp(-5 |\theta_1^2+\theta_2^2-1|)
	\end{equation}
	joka muodostaa regasmaisen 2-ulotteisen jakauman. Valitaan ehdotusjakaumaksi $J_n(\theta_n|\theta_{n-1})$ 2d-multinormaalijakauma 
	\begin{equation}
		J_n(\theta_n|\theta_{n-1}) \sim N(\theta_{n-1}, \sigma^2 I_2)
	\end{equation}
	jossa $I_2$ on 2x2 yksikkömatriisi ja olkoot $\sigma^2 = 0.01$. Nyt Metropolis Hastings algoritmin avulla voidaan simuloida jakaumaa $p(\theta)$ algoritmilla \ref{mh-maar}. Simuloidaan kaksi Markovin ketjua asettemalla aloitustiloiksi $(0,0)$ ja $(5,5)$, 
	\begin{figure}[h!]
		\includegraphics[width=\textwidth]{mhexample1}
		\caption[Kaksiulotteinen Metropolis--Hastings esimerkki]{\textit{Vasemmalla: (5,5). Keskellä: (0,0) Oikealla: tiheysestimaatti (huomaa eri skaala).}}
		\label{kuva1}
	\end{figure}
	kummastakin 10 000 tilaa. Simuloimme myös 200 000 pistettä aloitusarvolla (0,0), joista luodaan tiheysestimaatti. Tulokset löytyy kuvasta \ref{kuva1}. Kahdessa ekassa kuvassa viiva on ensimmäisen 250 pisteen polku. Selvästi nähdään, että aloituspisteellä ei ole väliä. Markovin ketjun tasapainojakauma on sama huolimatta aloituspisteestä.
	
\end{esim}

\begin{esim}\label{MH-esim2}
	\begin{figure}[h!]
		\includegraphics[width=0.7\textwidth]{mhexample2}
		\caption[Yksiulotteinen Metropolis--Hastings esimerkki]{Esimerkin \ref{MH-esim2} tulokset}
		\label{kuva2}
	\end{figure}
	Otetaan toisena esimerkkinä klassinen tapaus, jossa oletetaan, että havainnot ovat jauakautuneet \textit{Bernoulli-jakauman} mukaan, 
	$y_i \sim Bernoulli(\theta)$, 
	ja että priori on tasajakauma. Tällöin tiedetään, että analyyttinen posteriori on 
	\begin{equation*}
		p(\theta|y_i) = Beta(\sum_{i=1}^{n} y_i+1, n - \sum_{i=1}^{n} y_i+1)
	\end{equation*}
	Valitaan hieman eksoottinen ehdotusjakauma esimerkin vuoksi:
	\begin{equation}
		J_n(\theta_n|\theta_{n-1}) \sim \begin{cases}
			Unif(\theta_{n-1},1) & \text{kun } \theta_{n-1}<0.5 \\
			Unif(0,\theta_{n-1}) & \text{kun } \theta_{n-1}\geq0.5
		\end{cases}
	\end{equation}
	Toisin kuin esimerkissä \ref{mh-esim1}, nyt ehdotusjakauma ei olekkaan symmetrinen.
	
	Oletetaan että, meillä on havainnot $(1,1,1,0,0,1,0,0,0,0)$ ja tarkastellaan sekä analyyttistä että MH-algoritmin tuottamaa jakaumaa ja niiden eroja.

	Kuvasta \ref{kuva2} nähdään esimerkin tulokset. Huomataan, että vaikka ehdotusjakauma on melko kummallinen, niin kuitenkin tarpeaksi monella iteraatiolla saavutetaan tasapainojakauma. Huomaa, että vasemmassa kuvaajassa on vihreällä simulaatio keskiarvo, ja punaisella analyyttinen keskiarvo, mutta nämä arvot ovat niin lähellä toisiaan, että viivat ovat päällekkäin.
\end{esim}

\subsection{Ehdotusjakauman valinnasta}

Kummassakin kappaleen \ref{Metropolis--Hastings algoritmi} esimerkissä valitsimme ehdotusjakauman melko satunnaisesti. Varsinkin esimerkissä \ref{MH-esim2} se on erittäin epätavallinen, mistä syystä hyvän approksimaation saavuttaminen vie todella monta iteraatiota. Yleensä jos haluamme oikeasti tehokkaasti ja ekonomisesti simuloida jakaumia esitetyllä algoritmilla, haluamme valita ehdotusjakauman jollakin järkevällä, systemaattisella tavalla, joka minimoisi tarvittavien iteraatioiden määrän.

Yleisesti ottaen hyvällä ehdotusjakaumalla on muutama ominaisuus\cite{gelman_andrew_bayesian_nodate}
\begin{enumerate}
	\item Kaikilla $\theta$:n arvoilla on helppo arpoa arvo $J(\theta'|\theta)$
	\item Suhde $r$ on helppo laskea
	\item Siirtymät ovat tarpeaksi pitkiä. Muuten Markovin Ketju etenee liian hitaasti ja hyvän estimaatin saaminen kestää liian pitkään.
	\item Siirtymiä ei hylätä liian usein. Muuten Markovin Ketju ei etene vaan seisoo paikallaan.
\end{enumerate}

Lisäksi simulointia voidaan nopeuttaa mm. käyttämällä adaptiivista ehdotusjakaumaa, eli toisinsanoen ehdotusjakaumaa muunnellaan riippuen ketjun liikkeistä. 

















